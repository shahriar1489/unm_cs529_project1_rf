{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1737c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb 18 12:11:08 2024\n",
    "\n",
    "@author: shahriar\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# Arithmetic \n",
    "from fractions import Fraction\n",
    "from decimal import Decimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43c51e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "- Along the pipeline, keep the dataframe as pandas for as long as possible. \n",
    "\n",
    "- \n",
    "\n",
    "- \n",
    "\n",
    "- \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ID3: \n",
    "    \n",
    "    def __init__(self, df, attributes):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : training examples\n",
    "        \n",
    "        target_attribute : attriburte whose value is to be predicted by tree \n",
    "            \n",
    "        attributes : list of other attributes to be tested \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns a decision tree that correctly classsifies the examples\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        examples = df[:, 0:-1]\n",
    "        target_attribute = df[:, -1]\n",
    "        \n",
    "        # 1. create  a root for the tree\n",
    "        root = [] \n",
    "        \n",
    "        # 2 and 3. All examples have same label -> return single node with that label\n",
    "        target_values , target_counts = np.unique(target_attribute, return_counts=True)\n",
    "        \n",
    "        if len(target_values) ==1: \n",
    "            \n",
    "            return root, target_values[0] # I am not sure how this will look  \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 4.attributes is empty \n",
    "            # then return single-node tree root, \n",
    "            # with most common label in target_attribute in examples\n",
    "        \n",
    "        if len(attributes) == 0: \n",
    "            # Find the most frequent target value\n",
    "            target_value, target_count = np.unique(target_attribute, return_counts=True)\n",
    "            most_frequent_value = target_value[np.argmax(target_count)]\n",
    "            \n",
    "            return root, most_frequent_value\n",
    "        \n",
    "        # 5. Do this - this is where is information gain is calculated \n",
    "        \"\"\"\n",
    "        Find the information_gain for each attribute to decide the best att\n",
    "        \n",
    "        - A is the variable with the best attribute that best classifies the examples \n",
    "        - examples_vi \n",
    "        - \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        A = None\n",
    "        \n",
    "        highest_info_gain = -100000.0\n",
    "        \n",
    "        for a in attributes: # find the attribute with the highest information gain \n",
    "            \n",
    "            \"\"\"\n",
    "            There needs to be a methods to decide if the attribute is discrete or continuous.\n",
    "            For now, assume all are discrete. \n",
    "            \"\"\"\n",
    "            \n",
    "            #info_gain = self.information_gain( examples[a], target_attribute, 'entropy') # examples is pd df\n",
    "            info_gain = self.information_gain_for_discrete_attribute(examples[a], target_attribute, 'entropy')        \n",
    "            \n",
    "            if info_gain > highest_info_gain: \n",
    "                A = a\n",
    "                highest_info_gain = info_gain \n",
    "        \n",
    "        \n",
    "        print('A w/ highest info gain : ', A)\n",
    "        print('highest_info_gain : ', highest_info_gain)\n",
    "        \n",
    "        \n",
    "        # set root to A \n",
    "        root.append(A) \n",
    "\n",
    "        # get the unique values in A\n",
    "        vi_list_np = np.unique(examples[A]) # examples is pandas df\n",
    "        vi_with_root = [] \n",
    "        \n",
    "\n",
    "        vi_count = 0 \n",
    "            \n",
    "        \n",
    "        for vi in vi_list_np : \n",
    "\n",
    "            vi_count = vi_count+1 \n",
    "            \n",
    "            # Add a new branch below root, corresponding to the test A = v_i \n",
    "            vi_with_root.append(A+'->'+vi) # a list \n",
    "            root.append( vi_with_root )\n",
    "            \n",
    "            # Let Examples_vi be the subest of examples that have value v_i for A \n",
    "            examples_vi  = examples[ examples[A] == vi ]\n",
    "            \n",
    "    \n",
    "        \n",
    "            # If examples_vi is empty : What I understand from this is that there is not tuple \n",
    "            \"\"\"\n",
    "            When examples_vi is empty, it means there is not tuple. But, \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            if examples_vi.empty: \n",
    "                \n",
    "                # Then below this new branch add a leaf node with label = most common value of Target_attribute in Examples\n",
    "                target_value, target_count = np.unique(target_attribute, return_counts=True)\n",
    "                most_frequent_value = target_value[np.argmax(target_count)]\n",
    "                \n",
    "                return root, most_frequent_value\n",
    "                \n",
    "                \n",
    "            else: #  \n",
    "                ID3(df[ df[ A ] == vi], attributes.remove(A))\n",
    "                \n",
    "            \n",
    "            vi_with_root = [] \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            I am thinking of using dictionary as for the mean branching an attribute\n",
    "            \n",
    "            - Root is a list\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "        #A = None \n",
    "        #examples_vi = [] #None \n",
    "        \n",
    "        #branches = [] # the branches of the tree\n",
    "        \n",
    "        \n",
    "        #if len( branches_vi) == 0: \n",
    "            # Then below this new branch add a  leaf node with label = most common value of Target_attribute in Examples\n",
    "        \n",
    "        \n",
    "        #else:  # below this new branch add the subtree \n",
    "        #    ID3( examples_vi,  target_attribute, )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def information_gain( examples, target_attribute, attribute, impurity='gini'): \n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : numpy array \n",
    "            the feature columns whose information gain is to be calculated \n",
    "            \n",
    "        target_attribute : numpy array \n",
    "            the target label - y\n",
    "            \n",
    "        attribute : string \n",
    "            the attribute whose info gain to be calculated \n",
    "        \n",
    "        impurity : string \n",
    "            the impurity measure - gini or entropy \n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy scalar real number \n",
    "\n",
    "        \"\"\"\n",
    "        return -1 \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "      \n",
    "        \n",
    "    def compute_impurity_by_label(self, attribute, impurity='gini'): # Impurity of the total dataset : DONE\n",
    "        \n",
    "        \"\"\"\n",
    "        FEATURES: \n",
    "        \n",
    "        attribute : pandas df\n",
    "            the column whose entropy is to be calculated\n",
    "        \n",
    "        impurity : string \n",
    "            the impurity measure used- gini or entropty \n",
    "        \n",
    "        \n",
    "        Returns \n",
    "            np real scalar number \n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # get the total number of instances/rows in the dataset\n",
    "        N = attribute.shape[0]\n",
    "        \n",
    "        print('\\t\\t Number of rows in attribute param:', N)\n",
    "        #sys.exit(0)\n",
    "    \n",
    "        # get the count\n",
    "        label_values, label_counts = np.unique(attribute, return_counts=True)\n",
    "        label_fractions = []\n",
    "    \n",
    "    \n",
    "        # get the fractions for the each of the labels- better to use loop be cause there can be more than two labels\n",
    "    \n",
    "        for count in label_counts :\n",
    "            print(Decimal(count/N)) \n",
    "            \n",
    "            result_float = float( count/ Decimal(N) )\n",
    "            \n",
    "            label_fractions.append( result_float  )\n",
    "    \n",
    "    \n",
    "        print('\\t\\tlabel_fractions: ',label_fractions)\n",
    "        \n",
    "        label_fractions = np.array( label_fractions )\n",
    "        print('\\t\\tDifferent label values collected: ', label_values)\n",
    "        print('\\t\\tDifferent label counts colleceted: ', label_counts)\n",
    "        print('\\t\\tFractions of different labels: ', label_fractions)\n",
    "    \n",
    "    \n",
    "        # write a subroutine for entropy\n",
    "        if impurity=='entropy':\n",
    "            #return  - np.sum ( label_fractions * np.log2(  label_fractions ) ) # This returns the complete entropy \n",
    "            print('-------------\\n\\n\\n')\n",
    "            #print(\"\\t\\t\\tInside impurity=entropy\",  -1 * label_fractions * np.log2(label_fractions) ) \n",
    "    \n",
    "            print(\"-------------\\t\\t\\tnp.sum = \", -np.sum(  label_fractions * np.log2(label_fractions) ) )\n",
    "            \n",
    "            \n",
    "            return -np.sum(  label_fractions * np.log2(label_fractions) )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "        # write a subroutine for gini\n",
    "        elif impurity=='gini':  \n",
    "    \n",
    "          return 1 - np.sum(  np.square( label_fractions )   ) # 1 - sum of elementwise fraction #This returns the complete gini\n",
    "    \n",
    "    \n",
    "        else :\n",
    "    \n",
    "            print(\"ERROR: impurity metric can be either of gini or entropy.\")\n",
    "            return -1 \n",
    "        \n",
    "        \n",
    "    def information_gain_for_discrete_attribute(self, examples_a, target_attribute, impurity='entropy'): # 02/28/2024 This stays. Fix this \n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples_a : the attribute column whose feature is to be calculated \n",
    "            type: Pandas Series \n",
    "            \n",
    "        target_attribute : attribute whose value is to be predicted by tree \n",
    "            type: Pandas Series  \n",
    "        \n",
    "        attribute : attribute/column name for examples_a\n",
    "            type: string\n",
    "        \n",
    "        impurity_measure : gini/entropy \n",
    "            type: string\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scalar real number  \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.information_gain( examples[a], target_attribute, 'entropy') # examples is pd df\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #impurity_for_target_attribute = self.compute_impurity_for_discrete_attribute(target_attribute, impurity=impurity)\n",
    "        \n",
    "        \n",
    "        # get the unique values in examples_a\n",
    "        examples_a_values = np.unique(examples_a)\n",
    "        \n",
    "        N = examples_a.shape[0]\n",
    "        \n",
    "        result = self.compute_impurity_by_label(  target_attribute=target_attribute , impurity=impurity)\n",
    "        \n",
    "        print( '\\t\\t\\tresult after initialization : ', result) # ok \n",
    "        \n",
    "        #sys.exit(0)\n",
    "        for a in examples_a_values: \n",
    "            \n",
    "            # get the subset of examples_a and corresponding tuple in target_attribute\n",
    "            #examples_a[attribute]\n",
    "            #print( examples_a[examples_a==a])\n",
    "            #print('-----')\n",
    "            #print('feature subset shape:\\n', examples_a[examples_a==a].shape)\n",
    "            #print('-----')\n",
    "            \n",
    "            #print( 'target subset shape:\\n', target_attribute[examples_a==a].shape )\n",
    "        \n",
    "            \n",
    "            #examples_a_subset = np.array( examples_a[examples_a==a] ) \n",
    "            \"\"\"\n",
    "            I don't need the line above rn\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #target_a_subset = np.array( target_attribute[examples_a==a] ) # converting to np for faster computation\n",
    "            \n",
    "            n = target_attribute[examples_a==a].shape[0]\n",
    "            #compute_impurity_by_label(  np.array( target_attribute[examples_a==a] ), impurity=impurity)\n",
    "            \n",
    "            \n",
    "            prob_float = float( n/ Decimal(N) )\n",
    "            \n",
    "            \n",
    "            impurity_a = self.compute_impurity_by_label( target_attribute[examples_a==a] , impurity=impurity) * prob_float\n",
    "            \n",
    "            result = result - impurity_a\n",
    "            \n",
    "            print('\\t\\t---------------\\t\\t\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('\\t\\t\\t--- final info gain : ', result )\n",
    "            \n",
    "        return result # returns a scalar real number         \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#!/\n",
    "\n",
    "class ID3: \n",
    "    \n",
    "    def __init__(self, df, attributes):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : training examples\n",
    "        \n",
    "        target_attribute : attriburte whose value is to be predicted by tree \n",
    "            \n",
    "        attributes : list of other attributes to be tested \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns a decision tree that correctly classsifies the examples\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        examples = df.iloc[:, 0:-1]\n",
    "        target_attribute = df.iloc[:, -1]\n",
    "        \n",
    "        # 1. create  a root for the tree\n",
    "        root = [] \n",
    "        \n",
    "        # 2 and 3. All examples have same label -> return single node with that label\n",
    "        target_values , target_counts = np.unique(target_attribute, return_counts=True)\n",
    "        \n",
    "        if len(target_values) ==1: \n",
    "            \n",
    "            return root, target_values[0] # I am not sure how this will look  \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 4.attributes is empty \n",
    "            # then return single-node tree root, \n",
    "            # with most common label in target_attribute in examples\n",
    "        \n",
    "        if len(attributes) == 0: \n",
    "            # Find the most frequent target value\n",
    "            target_value, target_count = np.unique(target_attribute, return_counts=True)\n",
    "            most_frequent_value = target_value[np.argmax(target_count)]\n",
    "            \n",
    "            \n",
    "            return root, most_frequent_value\n",
    "        \n",
    "        # 5. Do this - this is where is information gain is calculated \n",
    "        \"\"\"\n",
    "        Find the information_gain for each attribute to decide the best att\n",
    "        \n",
    "        - A is the variable with the best attribute that best classifies the examples \n",
    "        - examples_vi \n",
    "        - \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        A = None # This is th attribute with the highest info gain \n",
    "        \n",
    "        highest_info_gain = -100000.0\n",
    "        \n",
    "        for a in attributes: # find the attribute with the highest information gain \n",
    "            \n",
    "            \"\"\"\n",
    "            There needs to be a methods to decide if the attribute is discrete or continuous.\n",
    "            For now, assume all are discrete. \n",
    "            \"\"\"\n",
    "            \n",
    "            info_gain = self.information_gain( examples[a], target_attribute, 'entropy') # examples is pd df\n",
    "                    \n",
    "            \n",
    "            if info_gain > highest_info_gain: \n",
    "                A = a\n",
    "                highest_info_gain = info_gain \n",
    "        \n",
    "        # set root to A \n",
    "        root.append(A) \n",
    "\n",
    "        print('root : ', root)\n",
    "        print('highest_info_gain : ', highest_info_gain)\n",
    "        #sys.exit()\n",
    "        \n",
    "        \n",
    "        # get the unique values in A\n",
    "        vi_list_np = np.unique(examples[A]) # examples is pandas df\n",
    "        vi_with_root = [] \n",
    "        \n",
    "        \n",
    "        sys.exit(0)\n",
    "\n",
    "        vi_count = 0 \n",
    "            \n",
    "        \n",
    "        for vi in vi_list_np : \n",
    "\n",
    "            vi_count = vi_count+1 \n",
    "            \n",
    "            # Add a new branch below root, corresponding to the test A = v_i \n",
    "            vi_with_root.append(A+'->'+vi) # a list \n",
    "            root.append( vi_with_root )\n",
    "            \n",
    "            # Let Examples_vi be the subest of examples that have value v_i for A \n",
    "            examples_vi  = examples[ examples[A] == vi ]\n",
    "            \n",
    "    \n",
    "        \n",
    "            # If examples_vi is empty : What I understand from this is that there is not tuple \n",
    "            \"\"\"\n",
    "            When examples_vi is empty, it means there is not tuple. But, \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            if examples_vi.empty: \n",
    "                \n",
    "                # Then below this new branch add a leaf node with label = most common value of Target_attribute in Examples\n",
    "                target_value, target_count = np.unique(target_attribute, return_counts=True)\n",
    "                most_frequent_value = target_value[np.argmax(target_count)]\n",
    "                \n",
    "                return root, most_frequent_value\n",
    "                \n",
    "                \n",
    "            else: #  \n",
    "                ID3(df[ df[ A ] == vi], attributes.remove(A))\n",
    "                \n",
    "            \n",
    "            vi_with_root = [] \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            I am thinking of using dictionary as for the mean branching an attribute\n",
    "            \n",
    "            - Root is a list\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "        #A = None \n",
    "        #examples_vi = [] #None \n",
    "        \n",
    "        #branches = [] # the branches of the tree\n",
    "        \n",
    "        \n",
    "        #if len( branches_vi) == 0: \n",
    "            # Then below this new branch add a  leaf node with label = most common value of Target_attribute in Examples\n",
    "        \n",
    "        \n",
    "        #else:  # below this new branch add the subtree \n",
    "        #    ID3( examples_vi,  target_attribute, )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def information_gain( examples, target_attribute, attribute, impurity='gini'): \n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : numpy array \n",
    "            the feature columns whose information gain is to be calculated \n",
    "            \n",
    "        target_attribute : numpy array \n",
    "            the target label - y\n",
    "            \n",
    "        attribute : string \n",
    "            the attribute whose info gain to be calculated \n",
    "        \n",
    "        impurity : string \n",
    "            the impurity measure - gini or entropy \n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy scalar real number \n",
    "\n",
    "        \"\"\"\n",
    "        return -1 \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "      \n",
    "        \n",
    "    def compute_impurity_by_label(self, attribute, impurity='gini'): # Impurity of the total dataset : DONE\n",
    "        \n",
    "        \"\"\"\n",
    "        FEATURES: \n",
    "        \n",
    "        attribute : pandas df\n",
    "            the column whose entropy is to be calculated\n",
    "        \n",
    "        impurity : string \n",
    "            the impurity measure used- gini or entropty \n",
    "        \n",
    "        \n",
    "        Returns \n",
    "            np real scalar number \n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # get the total number of instances/rows in the dataset\n",
    "        N = attribute.shape[0]\n",
    "        \n",
    "        print('\\t\\t Number of rows in attribute param:', N)\n",
    "        #sys.exit(0)\n",
    "    \n",
    "        # get the count\n",
    "        label_values, label_counts = np.unique(attribute, return_counts=True)\n",
    "        label_fractions = []\n",
    "    \n",
    "    \n",
    "        # get the fractions for the each of the labels- better to use loop be cause there can be more than two labels\n",
    "    \n",
    "        for count in label_counts :\n",
    "            print(Decimal(count/N)) \n",
    "            \n",
    "            result_float = float( count/ Decimal(N) )\n",
    "            \n",
    "            label_fractions.append( result_float  )\n",
    "    \n",
    "    \n",
    "        print('\\t\\tlabel_fractions: ',label_fractions)\n",
    "        \n",
    "        label_fractions = np.array( label_fractions )\n",
    "        print('\\t\\tDifferent label values collected: ', label_values)\n",
    "        print('\\t\\tDifferent label counts colleceted: ', label_counts)\n",
    "        print('\\t\\tFractions of different labels: ', label_fractions)\n",
    "    \n",
    "    \n",
    "        # write a subroutine for entropy\n",
    "        if impurity=='entropy':\n",
    "            #return  - np.sum ( label_fractions * np.log2(  label_fractions ) ) # This returns the complete entropy \n",
    "            print('-------------\\n\\n\\n')\n",
    "            #print(\"\\t\\t\\tInside impurity=entropy\",  -1 * label_fractions * np.log2(label_fractions) ) \n",
    "    \n",
    "            print(\"-------------\\t\\t\\tnp.sum = \", -np.sum(  label_fractions * np.log2(label_fractions) ) )\n",
    "            \n",
    "            \n",
    "            return -np.sum(  label_fractions * np.log2(label_fractions) )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "        # write a subroutine for gini\n",
    "        elif impurity=='gini':  \n",
    "    \n",
    "          return 1 - np.sum(  np.square( label_fractions )   ) # 1 - sum of elementwise fraction #This returns the complete gini\n",
    "    \n",
    "    \n",
    "        else :\n",
    "    \n",
    "            print(\"ERROR: impurity metric can be either of gini or entropy.\")\n",
    "            return -1 \n",
    "        \n",
    "        \n",
    "    def information_gain_for_discrete_attribute(self, examples_a, target_attribute, impurity='entropy'): # 02/28/2024 This stays. Fix this \n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples_a : the attribute column whose feature is to be calculated \n",
    "            type: Pandas Series \n",
    "            \n",
    "        target_attribute : attribute whose value is to be predicted by tree \n",
    "            type: Pandas Series  \n",
    "        \n",
    "        attribute : attribute/column name for examples_a\n",
    "            type: string\n",
    "        \n",
    "        impurity_measure : gini/entropy \n",
    "            type: string\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scalar real number  \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.information_gain( examples[a], target_attribute, 'entropy') # examples is pd df\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #impurity_for_target_attribute = self.compute_impurity_for_discrete_attribute(target_attribute, impurity=impurity)\n",
    "        \n",
    "        \n",
    "        # get the unique values in examples_a\n",
    "        examples_a_values = np.unique(examples_a)\n",
    "        \n",
    "        N = examples_a.shape[0]\n",
    "        \n",
    "        result = self.compute_impurity_by_label(  target_attribute=target_attribute , impurity=impurity)\n",
    "        \n",
    "        print( '\\t\\t\\tresult after initialization : ', result) # ok \n",
    "        \n",
    "        #sys.exit(0)\n",
    "        for a in examples_a_values: \n",
    "            \n",
    "            # get the subset of examples_a and corresponding tuple in target_attribute\n",
    "            #examples_a[attribute]\n",
    "            #print( examples_a[examples_a==a])\n",
    "            #print('-----')\n",
    "            #print('feature subset shape:\\n', examples_a[examples_a==a].shape)\n",
    "            #print('-----')\n",
    "            \n",
    "            #print( 'target subset shape:\\n', target_attribute[examples_a==a].shape )\n",
    "        \n",
    "            \n",
    "            #examples_a_subset = np.array( examples_a[examples_a==a] ) \n",
    "            \"\"\"\n",
    "            I don't need the line above rn\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #target_a_subset = np.array( target_attribute[examples_a==a] ) # converting to np for faster computation\n",
    "            \n",
    "            n = target_attribute[examples_a==a].shape[0]\n",
    "            #compute_impurity_by_label(  np.array( target_attribute[examples_a==a] ), impurity=impurity)\n",
    "            \n",
    "            \n",
    "            prob_float = float( n/ Decimal(N) )\n",
    "            \n",
    "            \n",
    "            impurity_a = self.compute_impurity_by_label( target_attribute[examples_a==a] , impurity=impurity) * prob_float\n",
    "            \n",
    "            result = result - impurity_a\n",
    "            \n",
    "            print('\\t\\t---------------\\t\\t\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('\\t\\t\\t--- final info gain : ', result )\n",
    "            \n",
    "        return result # returns a scalar real number         \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "010b936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree.py\n",
      ".DS_Store\n",
      "test_data_1.csv\n",
      "Untitled.ipynb\n",
      "id3.ipynb\n",
      "test.csv\n",
      "README.md\n",
      "discrete_attribute_test.py\n",
      "PlayTennis.csv\n",
      "train.csv\n",
      ".ipynb_checkpoints\n",
      ".git\n",
      "Data\n",
      "test.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# List all files and folders in the current directory\n",
    "files_and_folders = os.listdir(current_dir)\n",
    "\n",
    "# Print the list\n",
    "for item in files_and_folders:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "379a4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tennis = pd.read_csv('PlayTennis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fff4ea44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['no', 'yes'], dtype=object), array([5, 9]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique( df_tennis.play, return_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bc77f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTest different stages of ID3 on Play Tennis Dataset\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test different stages of ID3 on Play Tennis Dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f63cd752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['outlook', 'temp', 'humidity', 'windy', 'play'], dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tennis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6f5368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>overcast</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sunny</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>overcast</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outlook  temp humidity  windy play\n",
       "0      sunny   hot     high  False   no\n",
       "1      sunny   hot     high   True   no\n",
       "2   overcast   hot     high  False  yes\n",
       "3      rainy  mild     high  False  yes\n",
       "4      rainy  cool   normal  False  yes\n",
       "5      rainy  cool   normal   True   no\n",
       "6   overcast  cool   normal   True  yes\n",
       "7      sunny  mild     high  False   no\n",
       "8      sunny  cool   normal  False  yes\n",
       "9      rainy  mild   normal  False  yes\n",
       "10     sunny  mild   normal   True  yes\n",
       "11  overcast  mild     high   True  yes\n",
       "12  overcast   hot   normal  False  yes\n",
       "13     rainy  mild     high   True   no"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['outlook', 'temp', 'humidity', 'windy', 'play']\n",
    "attributes = ['outlook', 'temp', 'humidity', 'windy',] \n",
    "df_tennis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6d8fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes:  ['outlook', 'temp', 'humidity', 'windy']\n",
      "root ['outlook']\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahriar/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "print('attributes: ', attributes)\n",
    "#print()\n",
    "\n",
    "# Create an instance of the ID3 class\n",
    "id3_instance = ID3(df_tennis, attributes)\n",
    "\n",
    "# Call the __init__ method of the ID3 class\n",
    "#root, result = id3_instance.__init__(df_tennis.values, attributes)\n",
    "\n",
    "# Example usage:\n",
    "##print(\"Root of the decision tree:\", root)\n",
    "#print(\"Result of the decision tree:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06510eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tennis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdad7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_tennis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a31b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
